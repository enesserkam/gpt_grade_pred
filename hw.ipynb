{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/html/*.html\"\n",
    "\n",
    "code2convos = dict()\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(data_path))))\n",
    "for path in pbar:\n",
    "    # print(Path.cwd() / path)\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # get the file id to use it as key later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # read the html file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # parse the html file with bs4 so we can extract needed stuff\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # grab the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : convo[0].text\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        code2convos[file_code] = convo_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see one of the conversations\n",
    "pprint(code2convos[\"0031c86e-81f4-4eef-9e0e-28037abf9883\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do:\n",
    "- Prompt matching with questions\n",
    "- Feature Engineering\n",
    "- Question Grades preparation\n",
    "- Train/Test split\n",
    "- Fitting a model for predicting the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Matching\n",
    "> We want to match the prompts with the questions in the Homework Let's\n",
    "> do it with a simple term frequency vectorizing method. For each prompt,\n",
    "> we will come with a vector that represents it. We will do the same\n",
    "> thing with each of the homework questions. Then, we will calculate the\n",
    "> vectors distanance to do the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "code2prompts = defaultdict(list)\n",
    "for code , convos in code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"])\n",
    "            user_prompts.append(conv[\"text\"])\n",
    "    code2prompts[code] = user_prompts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"Initialize\n",
    "*   First make a copy of the notebook given to you as a starter.\n",
    "*   Make sure you choose Connect form upper right.\n",
    "*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "    \"\"\"Load training dataset (5 pts)\n",
    "    *  Read the .csv file with the pandas library\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Understanding the dataset & Preprocessing (15 pts)\n",
    "Understanding the Dataset: (5 pts)\n",
    "> - Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)\n",
    "> - Display variable names (both dependent and independent).\n",
    "> - Display the summary of the dataset. (Hint: You can use the **info** function)\n",
    "> - Display the first 5 rows from training dataset. (Hint: You can use the **head** function)\n",
    "Preprocessing: (10 pts)\n",
    "\n",
    "> - Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**\n",
    "\n",
    "> - Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Set X & y, split data (5 pts)\n",
    "\n",
    "*   Shuffle the dataset.\n",
    "*   Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\n",
    "*   Split training and test sets as 80% and 20%, respectively.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Features and Correlations (10 pts)\n",
    "\n",
    "* Correlations of features with health (4 points)\n",
    "Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\n",
    "\n",
    "* Feature Selection (3 points)\n",
    "Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\n",
    "\n",
    "* Hypothetical Driver Features (3 points)\n",
    "Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\n",
    "\n",
    "* __Note:__ You get can get help from GPT.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Tune Hyperparameters (20 pts)\n",
    "* Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under \"Parameters\" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)\n",
    "-Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Re-train and plot the decision tree with the hyperparameters you have chosen (15 pts)\n",
    "- Re-train model with the hyperparameters you have chosen in part 5). (10 pts)\n",
    "- Plot the tree you have trained. (5 pts)\n",
    "Hint: You can import the **plot_tree** function from the sklearn library.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Test your classifier on the test set (20 pts)\n",
    "- Predict the labels of testing data using the tree you have trained in step 6. (10 pts)\n",
    "- Report the classification accuracy. (2 pts)\n",
    "- Plot & investigate the confusion matrix. Fill the following blanks. (8 pts)\n",
    "> The model most frequently mistakes class(es) _________ for class(es) _________.\n",
    "Hint: You can use the confusion_matrix function from sklearn.metrics\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Find the information gain on the first split (10 pts)\"\"\",\n",
    "#####################\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "glove_input_file = 'glove/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove/glove.6B.100d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "glove_input_file = 'glove/glove.6B.50d.txt'\n",
    "word2vec_output_file = 'glove/glove.6B.50d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "glove_input_file = 'glove/glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove/glove.6B.200d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# glove_model = load('joblib_models/glove_model_50d.joblib')\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove/glove.6B.50d.word2vec.txt', binary=False)\n",
    "dump(glove_model, \"joblib_models/glove_model_50d.joblib\")\n",
    "def preprocess_and_tokenize(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return [word for word in words if word.isalpha()]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_prompts(prompts, model):\n",
    "    vectorized = []\n",
    "    for prompt in prompts:\n",
    "        words = preprocess_and_tokenize(prompt)\n",
    "        word_vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "\n",
    "        if len(word_vectors) == 0:\n",
    "            vectorized.append(np.zeros(model.vector_size))  \n",
    "        else:\n",
    "            vectorized.append(np.mean(word_vectors, axis=0)) \n",
    "\n",
    "    return pd.DataFrame(vectorized)\n",
    "\n",
    "code2prompts_glove = dict()\n",
    "\n",
    "for code, user_prompts in code2prompts.items():\n",
    "    if len(user_prompts) > 0:\n",
    "        vectorized_df = vectorize_prompts(user_prompts, glove_model)\n",
    "        code2prompts_glove[code] = vectorized_df\n",
    "    else:\n",
    "        print(f\"{code}.html has no prompts\")\n",
    "        \n",
    "print(code2prompts_glove[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code2prompts_glove[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = vectorizer.fit(prompts + questions)\n",
    "questions_TF_IDF = pd.DataFrame(vectorizer.transform(questions).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "questions_TF_IDF.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code2prompts_tf_idf = dict()\n",
    "for code, user_prompts in code2prompts.items():\n",
    "    if len(user_prompts) == 0:\n",
    "        print(code+\".html\")\n",
    "        continue\n",
    "    prompts_TF_IDF = pd.DataFrame(vectorizer.transform(user_prompts).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    code2prompts_tf_idf[code] = prompts_TF_IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2prompts_tf_idf[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2cosine = dict()\n",
    "for code, user_prompts_tf_idf in code2prompts_tf_idf.items():\n",
    "    code2cosine[code] = pd.DataFrame(cosine_similarity(questions_TF_IDF,user_prompts_tf_idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2cosine[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question_of_prompts = dict()\n",
    "\n",
    "for code, df in code2cosine.items():\n",
    "    max_indices = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        max_index = df[col].idxmax()\n",
    "\n",
    "        max_indices.append(max_index)\n",
    "\n",
    "    question_of_prompts[code] = max_indices\n",
    "    \n",
    "question_of_prompts[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code2prompts_glove[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_questions = len(questions)\n",
    "\n",
    "new_code_counts = dict()\n",
    "\n",
    "for code, indices in question_of_prompts.items():\n",
    "    counts = Counter(indices)\n",
    "\n",
    "    count_vector = [counts.get(i, 0) for i in range(num_questions)]\n",
    "\n",
    "    new_code_counts[code] = count_vector\n",
    "\n",
    "new_code_counts[\"04f91058-d0f8-4324-83b2-19c671f433dc\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vector_size = 50   \n",
    "\n",
    "final_df = pd.DataFrame(index=code2prompts_glove.keys(), columns=[f'q{i}_feature_{j}' for i in range(num_questions) for j in range(vector_size)])\n",
    "\n",
    "for code in code2prompts_glove.keys():\n",
    "    glove_vectors = code2prompts_glove[code]\n",
    "    question_indices = question_of_prompts[code]\n",
    "\n",
    "    summed_vectors = np.zeros((num_questions, vector_size))\n",
    "    prompt_counts = Counter(question_indices)\n",
    "\n",
    "    for vector, question_idx in zip(glove_vectors, question_indices):\n",
    "        scaled_vector = vector / prompt_counts[question_idx]\n",
    "        summed_vectors[question_idx] += scaled_vector\n",
    "\n",
    "    for i in range(num_questions):\n",
    "        column_labels = [f'q{i}_feature_{j}' for j in range(vector_size)]\n",
    "        final_df.loc[code, column_labels] = summed_vectors[i]\n",
    "\n",
    "final_df.rename_axis('code', inplace=True)\n",
    "final_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = scaler.fit_transform(final_df)\n",
    "\n",
    "normalized_df = pd.DataFrame(normalized_data, index=final_df.index, columns=final_df.columns)\n",
    "\n",
    "normalized_df.rename_axis('code', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Number of prompts that a user asked\n",
    "- Number of complaints that a user makes e.g \"the code gives this error!\"\n",
    "- User prompts average number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "(123, 459)"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = [0.1,0.05,0.15,0.05,0.1,0.2,0.15,0.1,0.1]\n",
    "\n",
    "modified_code_counts = {}\n",
    "\n",
    "for code, count_vector in new_code_counts.items():\n",
    "    modified_vector = [count * coeff for count, coeff in zip(count_vector, coefficients)]\n",
    "    modified_code_counts[code] = modified_vector\n",
    "\n",
    "modified_counts_df = pd.DataFrame.from_dict(modified_code_counts, orient='index')\n",
    "\n",
    "modified_counts_df.index.name = 'code'\n",
    "\n",
    "extended_df = normalized_df.merge(modified_counts_df, left_on='code', right_index=True)\n",
    "\n",
    "modified_count_col_names = [f'modified_count_{i}' for i in range(num_questions)]\n",
    "extended_df.rename(columns=dict(zip(range(num_questions), modified_count_col_names)), inplace=True)\n",
    "\n",
    "extended_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T18:34:04.502347Z",
     "start_time": "2024-01-19T18:34:04.485294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code2questionmapping = dict()\n",
    "for code, cosine_scores in code2cosine.items():\n",
    "    code2questionmapping[code] = code2cosine[code].max(axis=1).tolist()\n",
    "\n",
    "question_mapping_scores = pd.DataFrame(code2questionmapping).T\n",
    "question_mapping_scores.reset_index(inplace=True)\n",
    "question_mapping_scores.rename(columns={i: f\"Q_{i}\" for i in range(len(questions))}, inplace=True)\n",
    "question_mapping_scores.rename(columns={\"index\" : \"code\"}, inplace=True)\n",
    "\n",
    "question_mapping_scores.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df = question_mapping_scores.merge(extended_df, on='code', how='left')\n",
    "# merged_df = question_mapping_scores.merge(modified_counts_df, on='code', how='left')\n",
    "merged_df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the scores\n",
    "scores = pd.read_csv(\"data/scores.csv\", sep=\",\")\n",
    "scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "# selecting the columns we need and we care\n",
    "scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "# show some examples\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check grades distribution\n",
    "\n",
    "plt.title('Histogram Grades')\n",
    "plt.hist(scores[\"grade\"], rwidth=.8, bins=np.arange(min(scores[\"grade\"]), max(scores[\"grade\"])+2) - 0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging scores with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "temp_df = pd.merge(merged_df, scores, on='code', how=\"left\")\n",
    "temp_df.dropna(inplace=True)\n",
    "temp_df.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "\n",
    "temp_df = temp_df[temp_df['grade'] >= 70]\n",
    "\n",
    "temp_df['grade'] = temp_df['grade'] - 70\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "temp_df['grade'] = scaler.fit_transform(temp_df[['grade']])\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temp_df[temp_df.columns[1:-1]].to_numpy()\n",
    "y = temp_df[\"grade\"].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and Analyzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=0,criterion='squared_error', max_depth=5)\n",
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_MSEs = regressor.tree_.impurity   \n",
    "for idx, MSE in enumerate(regressor.tree_.impurity):\n",
    "    print(\"Node {} has MSE {}\".format(idx,MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculation of Mean Squared Error (MSE)\n",
    "print(\"MSE Train:\", mean_squared_error(y_train,y_train_pred))\n",
    "print(\"MSE TEST:\", mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train,y_train_pred))\n",
    "print(\"R2 TEST:\", r2_score(y_test,y_test_pred))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_regressor = Lasso(alpha=0.01, random_state=0)  \n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_lasso = lasso_regressor.predict(X_train)\n",
    "y_test_pred_lasso = lasso_regressor.predict(X_test)\n",
    "\n",
    "print(\"\\nLasso Regression:\")\n",
    "print(\"MSE Train:\", mean_squared_error(y_train, y_train_pred_lasso))\n",
    "print(\"MSE Test:\", mean_squared_error(y_test, y_test_pred_lasso))\n",
    "print(\"R2 Train:\", r2_score(y_train, y_train_pred_lasso))\n",
    "print(\"R2 Test:\", r2_score(y_test, y_test_pred_lasso))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "linear_reg = LinearRegression()\n",
    "pcr = make_pipeline(pca, linear_reg)\n",
    "\n",
    "pcr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_pcr = pcr.predict(X_train)\n",
    "y_test_pred_pcr = pcr.predict(X_test)\n",
    "\n",
    "print(\"\\nPrincipal Component Regression:\")\n",
    "print(\"MSE Train:\", mean_squared_error(y_train, y_train_pred_pcr))\n",
    "print(\"MSE Test:\", mean_squared_error(y_test, y_test_pred_pcr))\n",
    "print(\"R2 Train:\", r2_score(y_train, y_train_pred_pcr))\n",
    "print(\"R2 Test:\", r2_score(y_test, y_test_pred_pcr))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "random_forest_regressor = RandomForestRegressor(random_state=42, n_estimators=40)  # n_estimators can be adjusted\n",
    "\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf = random_forest_regressor.predict(X_train)\n",
    "y_test_pred_rf = random_forest_regressor.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Regressor:\")\n",
    "print(\"MSE Train:\", mean_squared_error(y_train, y_train_pred_rf))\n",
    "print(\"MSE Test:\", mean_squared_error(y_test, y_test_pred_rf))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train, y_train_pred_rf))\n",
    "print(\"R2 Test:\", r2_score(y_test, y_test_pred_rf))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, almost all combinations have been tried.\n",
    "PCA+RF, RF+Lasso, Lasso+RF, RF + Average, PCR + Average and etc.\n",
    "\"\"\"\n",
    "\n",
    "X_train_ensemble = np.column_stack((y_train_pred_rf, y_train_pred_lasso)) \n",
    "X_test_ensemble = np.column_stack((y_test_pred_rf, y_test_pred_lasso))\n",
    "\n",
    "ensemble_regressor = DecisionTreeRegressor(random_state=42)\n",
    "ensemble_regressor.fit(X_train_ensemble, y_train)\n",
    "\n",
    "y_train_pred_ensemble = ensemble_regressor.predict(X_train_ensemble)\n",
    "y_test_pred_ensemble = ensemble_regressor.predict(X_test_ensemble)\n",
    "\n",
    "print(\"Ensemble Model with Decision Tree Regressor:\")\n",
    "print(\"MSE Train:\", mean_squared_error(y_train, y_train_pred_ensemble))\n",
    "print(\"MSE Test:\", mean_squared_error(y_test, y_test_pred_ensemble))\n",
    "print(\"R2 Train:\", r2_score(y_train, y_train_pred_ensemble))\n",
    "print(\"R2 Test:\", r2_score(y_test, y_test_pred_ensemble))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
